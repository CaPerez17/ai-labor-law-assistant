"""
Servicio de IA para Generaci√≥n de Respuestas
-----------------------------------------
Este m√≥dulo implementa la integraci√≥n con modelos GPT para generar
respuestas a consultas sobre derecho laboral.

Mejoras implementadas en optimizaci√≥n de prompt:
1. Instrucciones m√°s expl√≠citas sobre usar SOLO informaci√≥n de documentos proporcionados
2. Estructura mejorada para las respuestas legales con secciones claramente definidas
3. Mejor formato para las referencias legales y citaciones
4. Manejo optimizado de casos con informaci√≥n insuficiente
5. Evaluaci√≥n de confianza m√°s precisa
6. Optimizaci√≥n radical de consumo de tokens y manejo de cach√©
"""

import os
import json
import re
import time
import logging
import hashlib
from functools import lru_cache
from typing import List, Dict, Any, Optional, Tuple, Union
from openai import OpenAI
from datetime import datetime

import sys
from pathlib import Path
# Asegurar que backend/ est√© en sys.path para poder importar el m√≥dulo config
backend_dir = Path(__file__).resolve().parent.parent.parent
if str(backend_dir) not in sys.path:
    sys.path.append(str(backend_dir))

from config import (
    OPENAI_API_KEY, GPT_MODEL, validate_config, 
    ECONOMY_MODE, MAX_TOKENS_OUTPUT, MAX_DOCUMENTS, 
    MAX_CHARS_PER_DOC, ENABLE_CACHE, CACHE_DIR
)
from app.schemas.legal_document import LegalDocumentResponse
from app.schemas.query import QueryResponse, QueryStatus

# Configurar logging
logger = logging.getLogger("ai_service")

# Cache en memoria para respuestas
@lru_cache(maxsize=50)
def get_cached_response(query_hash: str) -> Optional[Dict[str, Any]]:
    """
    Recupera una respuesta desde la cach√© basada en el hash de la consulta
    
    Args:
        query_hash: Hash MD5 de la consulta
        
    Returns:
        Datos de respuesta cacheados o None si no existe
    """
    try:
        cache_file = os.path.join(CACHE_DIR, f"{query_hash}.json")
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"‚úÖ Respuesta recuperada de cach√©: {cache_file}")
                return data
    except Exception as e:
        logger.error(f"‚ùå Error al leer cach√©: {str(e)}")
    
    return None

def save_to_cache(query_hash: str, response_data: Dict[str, Any]) -> bool:
    """
    Guarda una respuesta en la cach√©
    
    Args:
        query_hash: Hash MD5 de la consulta
        response_data: Datos a guardar
        
    Returns:
        True si se guard√≥ correctamente, False en caso contrario
    """
    try:
        cache_file = os.path.join(CACHE_DIR, f"{query_hash}.json")
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(response_data, f, ensure_ascii=False, indent=2)
            logger.info(f"‚úÖ Respuesta guardada en cach√©: {cache_file}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Error al guardar en cach√©: {str(e)}")
        return False

def create_query_hash(query_text: str, search_results: List[Dict[str, Any]]) -> str:
    """
    Crea un hash √∫nico para una consulta y sus resultados de b√∫squeda
    
    Args:
        query_text: Texto de la consulta
        search_results: Resultados de b√∫squeda (se usa solo IDs y relevancia)
        
    Returns:
        Hash MD5 como string
    """
    # Crear una representaci√≥n simplificada de los resultados que capture
    # lo esencial pero ignore detalles que no afectan la respuesta
    simplified_results = []
    for doc in search_results[:MAX_DOCUMENTS]:
        simplified_results.append({
            "id": doc.get("id", ""),
            "relevance": doc.get("relevance_score", 0)
        })
    
    # Crear string para hash
    hash_input = f"{query_text}|{json.dumps(simplified_results)}"
    
    # Generar hash
    return hashlib.md5(hash_input.encode('utf-8')).hexdigest()

class AIService:
    """Servicio para generaci√≥n de respuestas basadas en IA"""

    def __init__(self):
        """Inicializa el cliente de OpenAI y configura el modelo"""
        self.api_key = OPENAI_API_KEY
        self.model = GPT_MODEL
        self.client = None
        self.max_retries = 3
        self.retry_delay = 2  # segundos
        
        # Intentar inicializar el cliente
        self._initialize_client()
        
    def _initialize_client(self) -> bool:
        """
        Inicializa el cliente de OpenAI.
        
        Returns:
            bool: True si la inicializaci√≥n fue exitosa, False si hubo un error
        """
        if not self.api_key or self.api_key in ["your_openai_api_key_here", "sk-your-actual-openai-api-key"]:
            logger.error("‚ùå OPENAI_API_KEY no configurada o inv√°lida")
            return False
            
        try:
            self.client = OpenAI(api_key=self.api_key)
            return True
        except Exception as e:
            logger.error(f"‚ùå Error al inicializar el cliente de OpenAI: {str(e)}")
            return False
    
    def is_api_key_valid(self) -> bool:
        """
        Verifica si la API key es v√°lida haciendo una petici√≥n m√≠nima.
        
        Returns:
            bool: True si la API key es v√°lida, False si no lo es
        """
        if not self.client:
            return False
            
        try:
            # Hacer una petici√≥n m√≠nima para verificar la API key
            response = self.client.models.list()
            return True
        except Exception as e:
            logger.error(f"‚ùå Error al verificar la API key: {str(e)}")
            return False
        
    def format_bm25_context(self, query_text: str, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Formatea los resultados de BM25 en un contexto estructurado y optimizado para GPT.
        
        Args:
            query_text: Texto de la consulta del usuario
            search_results: Lista de documentos relevantes recuperados con BM25
            
        Returns:
            Contexto estructurado como diccionario para enviar a GPT
        """
        # Usar configuraci√≥n optimizada para ahorrar tokens
        max_docs = MAX_DOCUMENTS
        max_chars = MAX_CHARS_PER_DOC
        
        # Limitar a los m√°s relevantes
        top_results = search_results[:max_docs]
        
        # Crear contexto estructurado
        formatted_documents = []
        for i, doc in enumerate(top_results, 1):
            # Extraer identificadores para citas
            doc_id = f"[Doc{i}]"
            legal_reference = ""
            
            # Determinar tipo de referencia basado en document_type
            doc_type = str(doc.get("document_type", "")).lower()
            if "ley" in doc_type:
                legal_reference = f"Ley-{doc.get('reference_number', 'N/A')}"
            elif "decreto" in doc_type:
                legal_reference = f"Decreto-{doc.get('reference_number', 'N/A')}"
            elif "sentencia" in doc_type:
                legal_reference = f"Sentencia-{doc.get('reference_number', 'N/A')}"
            elif "c√≥digo" in doc_type or "cst" in doc_type.lower():
                legal_reference = f"CST-{doc.get('reference_number', 'N/A')}"
            else:
                legal_reference = doc.get('reference_number', 'N/A')
            
            # Crear documento estructurado con snippet y contenido reducido
            content = doc.get("snippet", "")[:max_chars]
            
            formatted_doc = {
                "id": doc_id,
                "titulo": doc.get("title", "Documento sin t√≠tulo"),
                "referencia": legal_reference,
                "contenido": content
            }
            
            formatted_documents.append(formatted_doc)
        
        # Crear contexto optimizado
        context = {
            "consulta": query_text,
            "documentos": formatted_documents
        }
        
        return context
        
    def generate_gpt_response(
        self, 
        prompt: str, 
        system_message: str, 
        max_tokens: int = MAX_TOKENS_OUTPUT,
        temperature: float = 0.1,  # Reducido para respuestas m√°s concisas
        model: str = None,
        timeout: int = 30  # Reducido a 30 segundos
    ) -> Tuple[Optional[str], Optional[str]]:
        """
        Genera una respuesta utilizando la API de OpenAI con manejo de errores y reintentos.
        Optimizado para reducir consumo de tokens.
        
        Args:
            prompt: Prompt para enviar a GPT
            system_message: Mensaje del sistema para establecer el rol
            max_tokens: N√∫mero m√°ximo de tokens en la respuesta
            temperature: Temperatura para controlar la creatividad (0-1)
            model: Modelo a utilizar (si es None, se usa el predeterminado)
            timeout: Tiempo m√°ximo de espera para la respuesta en segundos
            
        Returns:
            Tupla con (respuesta, error)
            - Si hay √©xito: (respuesta, None)
            - Si hay error: (None, mensaje_error)
        """
        if not self.client:
            if not self._initialize_client():
                return None, "No se pudo inicializar el cliente de OpenAI. Verifica tu API key."
        
        # Usar el modelo especificado o el predeterminado
        model_to_use = model or self.model
        
        # Limpiar y truncar prompts si son muy largos
        if len(prompt) > 4000:  # Reducci√≥n dr√°stica de contexto
            logger.warning(f"‚ö†Ô∏è Prompt demasiado largo ({len(prompt)} caracteres). Truncando...")
            prompt = prompt[:4000]
        
        attempts = 0
        while attempts < self.max_retries:
            try:
                logger.info(f"üîÑ Enviando solicitud a OpenAI (intento {attempts+1}/{self.max_retries})")
                start_time = time.time()
                
                response = self.client.chat.completions.create(
                    model=model_to_use,
                    messages=[
                        {"role": "system", "content": system_message},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens,
                    timeout=timeout
                )
                
                execution_time = time.time() - start_time
                logger.info(f"‚úÖ Respuesta generada en {execution_time:.2f} segundos")
                
                return response.choices[0].message.content.strip(), None
                
            except Exception as e:
                error_str = str(e)
                logger.error(f"‚ùå Error al generar respuesta: {error_str}")
                
                # Analizar tipo de error
                if "maximum context length" in error_str.lower():
                    # Reducir longitud del prompt a√∫n m√°s
                    prompt = prompt[:len(prompt)//2]
                    logger.warning("‚ö†Ô∏è Reduciendo longitud del prompt para evitar exceder m√°ximo de tokens")
                elif "rate limit" in error_str.lower():
                    # Esperar m√°s tiempo en caso de l√≠mite de tasa
                    wait_time = (2 ** attempts) * self.retry_delay
                    logger.warning(f"‚ö†Ô∏è L√≠mite de tasa excedido. Esperando {wait_time} segundos...")
                    time.sleep(wait_time)
                elif "billing hard limit" in error_str.lower() or "quota" in error_str.lower():
                    logger.error("‚ùå Has excedido tu cuota de API. Verifica tu saldo y l√≠mites.")
                    return None, "Cuota de API excedida. Verifica tu saldo y l√≠mites en OpenAI."
                else:
                    logger.error(f"‚ùå Error desconocido: {error_str}")
                    return None, f"Error inesperado: {error_str}"
                
            attempts += 1
            
        return None, f"No se pudo completar la solicitud despu√©s de {self.max_retries} intentos."
        
    def generate_response(
        self, 
        query_text: str, 
        search_results: List[Dict[str, Any]],
        threshold: float = 0.7
    ) -> Tuple[str, float, bool, Optional[str]]:
        """
        Genera una respuesta a la consulta del usuario utilizando GPT.
        Versi√≥n optimizada para reducir consumo de tokens.
        
        Args:
            query_text: Texto de la consulta del usuario
            search_results: Documentos relevantes recuperados con BM25
            threshold: Umbral de confianza para decidir si se necesita revisi√≥n humana
            
        Returns:
            Tupla con:
            - Texto de respuesta generada
            - Puntuaci√≥n de confianza
            - Indicador de si necesita revisi√≥n humana
            - Raz√≥n de la revisi√≥n (opcional)
        """
        # Verificar que la API key es v√°lida
        if not validate_config():
            return (
                "Lo siento, hay un problema con la configuraci√≥n del sistema. Un especialista revisar√° tu caso.",
                0.0,
                True,
                "Error de configuraci√≥n de OpenAI API"
            )
        
        # Verificar cach√© si est√° habilitado
        if ENABLE_CACHE:
            query_hash = create_query_hash(query_text, search_results)
            cached_data = get_cached_response(query_hash)
            
            if cached_data:
                return (
                    cached_data["response"],
                    cached_data["confidence"],
                    cached_data["needs_review"],
                    cached_data["review_reason"]
                )
        
        # Si no hay suficientes resultados relevantes, devolver respuesta est√°ndar
        if not search_results or len(search_results) == 0:
            default_response = (
                "No tengo suficiente informaci√≥n en los documentos proporcionados para responder esta consulta de forma completa.\n\n"
                "**REFERENCIAS LEGALES:**\n"
                "- No se encontraron documentos relevantes a tu consulta.\n\n"
                "**CONFIANZA: 0.0**"
            )
            
            if ENABLE_CACHE:
                save_to_cache(query_hash, {
                    "response": default_response,
                    "confidence": 0.0,
                    "needs_review": True,
                    "review_reason": "Sin documentos relevantes"
                })
                
            return default_response, 0.0, True, "Sin documentos relevantes"
            
        # Formatear el contexto de BM25 para GPT (versi√≥n optimizada)
        context = self.format_bm25_context(query_text, search_results)
        
        # Optimizar el prompt para GPT (versi√≥n ultra concisa)
        system_prompt = """
        Asistente legal colombiano. Responde bas√°ndote SOLO en los documentos proporcionados.
        REGLAS: No inventes informaci√≥n. S√© breve y directo. Cita fuentes como [DocX].
        Si no tienes suficiente informaci√≥n, indica: "No tengo suficiente informaci√≥n en los documentos proporcionados".
        ESTRUCTURA: 1) Respuesta directa, 2) Referencias legales.
        Al final eval√∫a tu confianza (0-1): "CONFIANZA: [puntuaci√≥n]"
        """
        
        # Convertir contexto a formato JSON simplificado
        context_json = json.dumps(context, ensure_ascii=False)
        
        user_prompt = f"""
        CONSULTA: {query_text}
        
        DOCUMENTOS:
        {context_json}
        
        Responde usando solo estos documentos. Incluye citas [DocX].
        """
        
        # Generar respuesta usando el m√©todo con manejo de errores
        response_text, error = self.generate_gpt_response(
            prompt=user_prompt,
            system_message=system_prompt,
            max_tokens=MAX_TOKENS_OUTPUT,
            temperature=0.1
        )
        
        # Si hubo un error, devolver mensaje de error
        if error:
            logger.error(f"‚ùå Error al generar respuesta: {error}")
            
            error_response = (
                "Lo siento, no pude procesar tu consulta en este momento debido a limitaciones t√©cnicas.\n\n"
                "**CONFIANZA: 0.0**"
            )
            
            return (
                error_response,
                0.0,
                True,
                f"Error en la generaci√≥n de respuesta: {error}"
            )
        
        # Extraer la puntuaci√≥n de confianza
        confidence_score = self._extract_confidence_score(response_text)
        
        # Determinar si necesita revisi√≥n humana
        needs_human_review = confidence_score < threshold
        review_reason = None
        
        if needs_human_review:
            if confidence_score < 0.4:
                review_reason = "Informaci√≥n insuficiente en los documentos proporcionados"
            else:
                review_reason = "Informaci√≥n parcial que requiere verificaci√≥n"
            
        # Limpiar la respuesta para quitar la l√≠nea de confianza
        clean_response = self._clean_response(response_text)
        
        # Formatear las referencias para que sean m√°s legibles
        clean_response = self._format_legal_references(clean_response)
        
        # Guardar en cach√© si est√° habilitado
        if ENABLE_CACHE:
            save_to_cache(query_hash, {
                "response": clean_response,
                "confidence": confidence_score,
                "needs_review": needs_human_review,
                "review_reason": review_reason
            })
        
        return clean_response, confidence_score, needs_human_review, review_reason
    
    def _extract_confidence_score(self, response_text: str) -> float:
        """Extrae la puntuaci√≥n de confianza de la respuesta"""
        try:
            # Buscar el patr√≥n "CONFIANZA: [puntuaci√≥n]"
            match = re.search(r"CONFIANZA:\s*(0\.\d+|1\.0|1)", response_text)
            if match:
                return float(match.group(1))
            return 0.5  # Valor por defecto si no se encuentra
        except:
            return 0.5  # Valor por defecto en caso de error
    
    def _clean_response(self, response_text: str) -> str:
        """Elimina la l√≠nea de confianza de la respuesta"""
        return re.sub(r"\n?CONFIANZA:\s*(0\.\d+|1\.0|1)\s*$", "", response_text).strip()
    
    def _format_legal_references(self, response_text: str) -> str:
        """Mejora el formato de las referencias legales"""
        # Destacar referencias a documentos
        formatted_text = re.sub(r'\[Doc(\d+)\]', r'[üìÑ Doc\1]', response_text)
        
        # Destacar referencias a art√≠culos
        formatted_text = re.sub(r'(art√≠culo|art\.|art)\s+(\d+)', r'Art√≠culo \2', formatted_text, flags=re.IGNORECASE)
        
        # Destacar referencias a leyes
        formatted_text = re.sub(r'(ley)\s+(\d+)(\s+de\s+\d{4})?', r'Ley \2\3', formatted_text, flags=re.IGNORECASE)
        
        return formatted_text
    
    def extract_document_citations(self, response_text: str) -> List[str]:
        """Extrae referencias a documentos de la respuesta"""
        citations = []
        matches = re.findall(r'\[Doc(\d+)\]', response_text)
        
        for match in matches:
            doc_num = match
            if f"Doc{doc_num}" not in citations:
                citations.append(f"Doc{doc_num}")
                
        return citations
    
    def format_response_with_sources(self, response_text: str, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Formatea la respuesta final incluyendo fuentes utilizadas
        
        Args:
            response_text: Texto de respuesta generado por GPT
            search_results: Resultados originales de la b√∫squeda
            
        Returns:
            Diccionario con respuesta y metadatos
        """
        # Extraer citas de documentos usadas en la respuesta
        citations = self.extract_document_citations(response_text)
        
        # Mapear documentos citados
        sources = []
        for i, doc in enumerate(search_results[:len(citations)], 1):
            if f"Doc{i}" in citations:
                sources.append({
                    "id": doc.get("document_id", i),
                    "title": doc.get("title", f"Documento {i}"),
                    "reference": doc.get("reference_number", "N/A"),
                    "relevance": doc.get("relevance_score", 0)
                })
        
        # Construir respuesta final
        formatted_response = {
            "response_text": response_text,
            "sources": sources,
            "metadata": {
                "total_documents": len(search_results),
                "documents_cited": len(sources),
                "timestamp": datetime.now().isoformat()
            }
        }
        
        return formatted_response

    def optimize_document_context(self, search_results: List[Dict[str, Any]], 
                                max_documents: int = 5, 
                                max_chars_per_doc: int = 2000) -> List[Dict[str, Any]]:
        """
        Optimiza los documentos para prepararlos para GPT, priorizando la informaci√≥n
        m√°s relevante y eliminando contenido redundante.
        
        Args:
            search_results: Resultados de b√∫squeda BM25
            max_documents: N√∫mero m√°ximo de documentos a incluir
            max_chars_per_doc: Longitud m√°xima de contenido por documento
            
        Returns:
            Lista de documentos optimizados para enviar a GPT
        """
        if not search_results:
            return []
        
        # Priorizar documentos m√°s relevantes
        top_results = sorted(search_results[:max_documents*2], key=lambda x: float(x.get("relevance_score", 0)), reverse=True)[:max_documents]
        
        # Eliminar documentos duplicados o con contenido muy similar
        seen_content = set()
        unique_docs = []
        
        for doc in top_results:
            # Crear una representaci√≥n simplificada del contenido para comparaci√≥n
            content_sample = doc.get("content", "")[:300].lower() if doc.get("content") else ""
            if not content_sample or hash(content_sample) not in seen_content:
                if content_sample:
                    seen_content.add(hash(content_sample))
                unique_docs.append(doc)
        
        # Crear contexto estructurado
        optimized_documents = []
        
        for i, doc in enumerate(unique_docs, 1):
            # Extraer identificadores para citas
            doc_id = f"[Doc{i}]"
            legal_reference = ""
            
            # Determinar tipo de referencia espec√≠fica basado en document_type
            doc_type = str(doc.get("document_type", "")).lower()
            ref_number = doc.get('reference_number', 'N/A')
            
            if "ley" in doc_type:
                legal_reference = f"Ley {ref_number}"
            elif "decreto" in doc_type:
                legal_reference = f"Decreto {ref_number}"
            elif "sentencia" in doc_type:
                legal_reference = f"Sentencia {ref_number}"
            elif "c√≥digo" in doc_type or "cst" in doc_type.lower():
                if "art" in ref_number.lower():
                    legal_reference = f"C√≥digo Sustantivo del Trabajo, {ref_number}"
                else:
                    legal_reference = f"C√≥digo Sustantivo del Trabajo, Art√≠culo {ref_number}"
            elif "resoluci√≥n" in doc_type:
                legal_reference = f"Resoluci√≥n {ref_number}"
            elif "circular" in doc_type:
                legal_reference = f"Circular {ref_number}"
            else:
                legal_reference = ref_number
            
            # Limpiar y formatear el contenido
            content = doc.get("content", "").strip()
            if not content:
                content = doc.get("snippet", "").strip()
            
            # Extraer las partes m√°s relevantes del contenido
            if len(content) > max_chars_per_doc:
                # Dividir en p√°rrafos
                paragraphs = [p.strip() for p in re.split(r'\n\s*\n', content) if p.strip()]
                
                # Priorizar p√°rrafos con palabras clave relacionadas con la consulta
                # (Esta es una simplificaci√≥n - en una implementaci√≥n m√°s completa
                # se podr√≠a usar embeddings o similaridad sem√°ntica)
                selected_content = []
                chars_count = 0
                
                # Asegurar que el primer p√°rrafo siempre se incluya (a menudo contiene definiciones)
                if paragraphs:
                    first_para = paragraphs[0]
                    selected_content.append(first_para)
                    chars_count += len(first_para)
                
                # A√±adir p√°rrafos restantes hasta alcanzar el l√≠mite
                for para in paragraphs[1:]:
                    if chars_count + len(para) + 1 <= max_chars_per_doc:
                        selected_content.append(para)
                        chars_count += len(para) + 1  # +1 por el separador
                    else:
                        break
                
                content = "\n\n".join(selected_content)
                if len(content) < len(doc.get("content", "")):
                    content += "..."
            
            # Crear documento estructurado
            optimized_doc = {
                "id": doc_id,
                "titulo": doc.get("title", "Documento sin t√≠tulo"),
                "referencia_legal": legal_reference,
                "tipo_documento": doc.get("document_type", "No especificado"),
                "relevancia": round(float(doc.get("relevance_score", 0)), 3),
                "contenido": content,
                "fuente_completa": doc.get("source", "No especificada"),
                "fecha_documento": doc.get("date", "No especificada")
            }
            
            optimized_documents.append(optimized_doc)
        
        return optimized_documents

    def generate_legal_response(
        self,
        query_text: str,
        search_results: List[Dict[str, Any]],
        max_documents: int = 5,
        max_tokens: int = 1500,
        confidence_threshold: float = 0.7,
        model: str = None,
        timeout: int = 60
    ) -> Tuple[str, float, List[Dict[str, Any]], bool]:
        """
        Genera una respuesta legal detallada a partir de una consulta y resultados de b√∫squeda BM25.
        
        Este m√©todo est√° optimizado para consultas legales que requieren citaci√≥n precisa
        de fuentes y alto nivel de precisi√≥n.
        
        Args:
            query_text: Consulta del usuario
            search_results: Resultados de b√∫squeda BM25
            max_documents: N√∫mero m√°ximo de documentos a incluir
            max_tokens: Tokens m√°ximos para la respuesta
            confidence_threshold: Umbral para determinar si una respuesta es confiable
            model: Modelo espec√≠fico a utilizar (si es None, usa el predeterminado)
            timeout: Tiempo m√°ximo de espera para la respuesta en segundos
            
        Returns:
            Tupla con:
            - respuesta_formateada: Texto de la respuesta 
            - puntuaci√≥n_confianza: Nivel de confianza (0-1)
            - documentos_citados: Lista de documentos usados en la respuesta
            - requiere_revision: Indicador de si la respuesta necesita revisi√≥n humana
        """
        if not search_results:
            return "No se encontraron documentos legales relevantes para tu consulta. Por favor, reformula tu pregunta o consulta a un especialista en derecho laboral.", 0.0, [], True
        
        # Verificar cliente OpenAI
        if not self.client and not self._initialize_client():
            logger.error("‚ùå No se pudo inicializar el cliente de OpenAI")
            return "Lo siento, hay un problema t√©cnico con nuestro servicio. Por favor, intenta m√°s tarde o contacta a un especialista en derecho laboral para tu consulta.", 0.0, [], True
        
        # Optimizar documentos para GPT
        optimized_docs = self.optimize_document_context(
            search_results=search_results,
            max_documents=max_documents
        )
        
        # Formatear documentos para JSON
        formatted_context = {
            "consulta_legal": query_text,
            "documentos_relevantes": optimized_docs,
            "total_documentos": len(search_results),
            "documentos_incluidos": len(optimized_docs),
            "timestamp": datetime.now().isoformat()
        }
        
        # Sistema de prompt optimizado para respuestas legales
        system_prompt = """
        Eres un asistente legal especializado en derecho laboral colombiano con amplia experiencia jur√≠dica.
        Tu √öNICA funci√≥n es proporcionar respuestas precisas basadas EXCLUSIVAMENTE en los documentos legales proporcionados.
        
        ## REGLAS FUNDAMENTALES (CR√çTICAS - DEBES SEGUIRLAS SIN EXCEPCI√ìN):
        
        1. NUNCA inventes informaci√≥n, leyes, art√≠culos o interpretaciones que NO aparezcan expl√≠citamente en los documentos proporcionados.
        2. NUNCA utilices tu conocimiento general sobre leyes - SOLO puedes usar la informaci√≥n que aparece en los documentos.
        3. Si los documentos proporcionados no contienen informaci√≥n suficiente, DEBES indicarlo claramente.
        4. SIEMPRE cita las fuentes espec√≠ficas usando el formato [DocX] despu√©s de cada afirmaci√≥n legal importante.
        5. SIEMPRE incluye referencias legales exactas (n√∫meros de ley, art√≠culos espec√≠ficos) tal como aparecen en los documentos.
        
        ## ESTRUCTURA OBLIGATORIA DE LA RESPUESTA:
        
        **RESPUESTA DIRECTA**:
        Comienza con una respuesta concisa y directa a la consulta legal.
        
        **FUNDAMENTO LEGAL**:
        Explica detalladamente el fundamento legal citando documentos espec√≠ficos.
        
        **REQUISITOS Y CONDICIONES** (si aplica):
        Enumera requisitos, plazos o condiciones especiales.
        
        **REFERENCIAS LEGALES**:
        Lista completa de documentos citados con sus referencias legales exactas.
        Ejemplo:
        ‚Ä¢ [Doc1] Ley 1010 de 2006, Art√≠culo 2 - Definici√≥n de acoso laboral
        ‚Ä¢ [Doc2] C√≥digo Sustantivo del Trabajo, Art√≠culo 62 - Terminaci√≥n del contrato
        
        ## CUANDO NO HAY INFORMACI√ìN SUFICIENTE:
        
        Si la informaci√≥n proporcionada es insuficiente para responder adecuadamente:
        1. Indica espec√≠ficamente qu√© informaci√≥n falta para responder completamente
        2. Proporciona la informaci√≥n parcial que S√ç est√° disponible en los documentos
        3. Asigna una puntuaci√≥n de confianza baja (0.0-0.3)
        4. Incluye este texto exacto al inicio: "ADVERTENCIA: La informaci√≥n disponible es limitada para responder completamente a esta consulta."
        
        ## EVALUACI√ìN DE CONFIANZA:
        
        Al final de tu respuesta, eval√∫a tu confianza en una escala de 0 a 1:
        - 0.0-0.3: Informaci√≥n muy insuficiente o no relevante
        - 0.4-0.6: Informaci√≥n parcial con algunas lagunas importantes
        - 0.7-0.9: Informaci√≥n suficiente con documentos relevantes
        - 1.0: Informaci√≥n completa y precisa con documentos altamente relevantes
        
        Al final, DEBES incluir tu evaluaci√≥n en el siguiente formato exacto:
        "CONFIANZA: [puntuaci√≥n]"
        """
        
        # Convertir contexto a formato JSON con indentaci√≥n para mejor legibilidad
        context_json = json.dumps(formatted_context, ensure_ascii=False, indent=2)
        
        # Crear prompt con instrucciones detalladas
        user_prompt = f"""
        ## CONSULTA LEGAL
        
        {query_text}
        
        ## DOCUMENTOS LEGALES RELEVANTES
        
        {context_json}
        
        ## INSTRUCCIONES ESPEC√çFICAS
        
        IMPORTANTE:
        - Responde bas√°ndote √öNICAMENTE en los documentos proporcionados.
        - Si no encuentras informaci√≥n suficiente, ind√≠calo claramente.
        - Cita las fuentes exactas para cada afirmaci√≥n legal.
        - Utiliza un lenguaje claro y accesible para personas sin formaci√≥n jur√≠dica.
        - Estructura tu respuesta seg√∫n el formato requerido.
        - Incluye la secci√≥n "REFERENCIAS LEGALES" con todas las fuentes utilizadas.
        
        Recuerda evaluar la confianza de tu respuesta al final.
        """
        
        # Generar respuesta usando el m√©todo con manejo de errores
        model_to_use = model or self.model
        response_text, error = self.generate_gpt_response(
            prompt=user_prompt,
            system_message=system_prompt,
            max_tokens=max_tokens,
            temperature=0.1,  # Temperatura m√°s baja para respuestas m√°s deterministas
            model=model_to_use,
            timeout=timeout
        )
        
        # Si hubo un error, devolver mensaje de error
        if error:
            logger.error(f"‚ùå Error al generar respuesta legal: {error}")
            return (
                f"Lo siento, no pude procesar tu consulta legal en este momento debido a un error t√©cnico. Por favor, intenta nuevamente m√°s tarde o consulta a un especialista en derecho laboral.",
                0.0,
                [],
                True
            )
        
        # Extraer la puntuaci√≥n de confianza
        confidence_score = self._extract_confidence_score(response_text)
        
        # Limpiar la respuesta y formatear referencias
        clean_response = self._clean_response(response_text)
        formatted_response = self._format_enhanced_legal_references(clean_response)
        
        # Determinar si requiere revisi√≥n humana
        requires_human_review = confidence_score < confidence_threshold
        
        # Extraer documentos citados
        citations = self.extract_document_citations(formatted_response)
        cited_documents = []
        
        for i, doc in enumerate(optimized_docs, 1):
            doc_id = f"Doc{i}"
            if doc_id in citations:
                cite_index = next((idx for idx, res in enumerate(search_results) 
                                if res.get("title") == doc.get("titulo")), None)
                
                if cite_index is not None:
                    original_doc = search_results[cite_index]
                    cited_documents.append({
                        "id": original_doc.get("document_id", i),
                        "title": original_doc.get("title", f"Documento {i}"),
                        "reference": original_doc.get("reference_number", "N/A"),
                        "relevance": original_doc.get("relevance_score", 0)
                    })
        
        # Si la confianza es muy baja, agregar disclaimer
        if confidence_score < 0.4:
            formatted_response = self._add_low_confidence_disclaimer(formatted_response)
        
        return formatted_response, confidence_score, cited_documents, requires_human_review

    def _format_enhanced_legal_references(self, response_text: str) -> str:
        """
        Mejora el formato de las referencias legales para mayor claridad y consistencia.
        
        Args:
            response_text: Texto de la respuesta generada
            
        Returns:
            Texto con referencias legales reformateadas
        """
        # Destacar referencias a documentos
        formatted_text = re.sub(r'\[Doc(\d+)\]', r'[üìÑ Doc\1]', response_text)
        
        # Destacar referencias a art√≠culos
        formatted_text = re.sub(
            r'(?i)(art√≠culo|art\.|art)\s+(\d+[a-z]?)(\s+del\s+(?:c√≥digo|cst|ley|decreto))?', 
            r'**Art√≠culo \2\3**', 
            formatted_text
        )
        
        # Destacar referencias a leyes
        formatted_text = re.sub(
            r'(?i)(ley)\s+(\d+)(\s+de\s+\d{4})?', 
            r'**Ley \2\3**', 
            formatted_text
        )
        
        # Destacar referencias a decretos
        formatted_text = re.sub(
            r'(?i)(decreto)\s+(\d+)(\s+de\s+\d{4})?', 
            r'**Decreto \2\3**', 
            formatted_text
        )
        
        # Destacar referencias a sentencias
        formatted_text = re.sub(
            r'(?i)(sentencia)\s+([a-z]-\d+)(\s+de\s+\d{4})?', 
            r'**Sentencia \2\3**', 
            formatted_text
        )
        
        # Formatear secci√≥n de referencias legales
        if "REFERENCIAS LEGALES" in formatted_text or "Referencias Legales" in formatted_text:
            # Buscar la secci√≥n de referencias legales usando regex
            pattern = r'(?:REFERENCIAS LEGALES|Referencias Legales)[:\s]*\n((?:.+\n)+)'
            match = re.search(pattern, formatted_text)
            
            if match:
                ref_section = match.group(0)
                formatted_ref_section = ref_section
                
                # Reformatear cada l√≠nea de la secci√≥n
                lines = ref_section.split('\n')
                formatted_lines = ["## " + lines[0].strip()]  # Destacar el t√≠tulo
                
                for line in lines[1:]:
                    if line.strip():
                        # Mejorar el formato de cada referencia
                        formatted_line = re.sub(r'^\s*-?\s*', '‚Ä¢ ', line)
                        formatted_line = re.sub(r'\[Doc(\d+)\]', r'[üìÑ Doc\1]', formatted_line)
                        formatted_lines.append(formatted_line)
                
                formatted_ref_section = '\n'.join(formatted_lines)
                formatted_text = formatted_text.replace(ref_section, formatted_ref_section)
        
        return formatted_text

    def _add_low_confidence_disclaimer(self, response_text: str) -> str:
        """
        Agrega un disclaimer a respuestas con baja confianza.
        
        Args:
            response_text: Texto de la respuesta generada
            
        Returns:
            Texto con disclaimer agregado
        """
        disclaimer = """
‚ö†Ô∏è **ADVERTENCIA IMPORTANTE**: Esta respuesta se basa en informaci√≥n limitada o parcial encontrada en los documentos disponibles. 
Es posible que no represente una orientaci√≥n legal completa. Para obtener asesoramiento legal preciso y adaptado a su situaci√≥n espec√≠fica, 
se recomienda consultar a un profesional especializado en derecho laboral.
"""
        
        return disclaimer + "\n\n" + response_text 